{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9f73235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models.addmul import HandleAddMul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f1725af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d525037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... FNN Network training on cuda:0 ...\n",
      "Accessing : networks/cache-networks/lyr256-split0.8-lr0.01-add-mul.data\n",
      "networks/cache-networks/lyr256-split0.8-lr0.01-add-mul.data\n",
      "Load saves ...\n"
     ]
    }
   ],
   "source": [
    "network_cache_dir = \"networks/cache-networks/\"\n",
    "network_name = \"lyr256-split0.8-lr0.01-add-mul.data\"\n",
    "\n",
    "checkpoint = True\n",
    "test_flag = 1\n",
    "\n",
    "input_dims = [42]\n",
    "output_dims = [20]\n",
    "batchsize = 128\n",
    "num_epochs = 1\n",
    "\n",
    "handler = HandleAddMul(input_dims, output_dims, dir=network_cache_dir + network_name, checkpoint=checkpoint, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3847fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = []\n",
    "for layer in handler.network.layers[0]:\n",
    "    if isinstance(layer, torch.nn.Linear):\n",
    "        logits.append(torch.full_like(layer.weight.data.clone(), 0.9, requires_grad=True))\n",
    "\n",
    "for name, param in handler.network.named_parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457d6cd4",
   "metadata": {},
   "source": [
    "## generate mask on addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d068d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = 0.8\n",
    "test_split = 1 - train_split\n",
    "\n",
    "data_fp = \"generate_datasets/tmp/digit-data/simple_add.npy\"\n",
    "data = np.load(data_fp, allow_pickle=True)\n",
    "\n",
    "data_len = len(data)\n",
    "train_split_idx = int(data_len * train_split)\n",
    "train_data = data[:train_split_idx]\n",
    "test_data = data[train_split_idx:]\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=torch.tensor(train_data), batch_size=batchsize, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=torch.Tensor(test_data), batch_size=batchsize, shuffle=True)\n",
    "\n",
    "def cycle(iterable):\n",
    "    while True:\n",
    "        for x in iterable:\n",
    "            yield x\n",
    "\n",
    "iterator_train = iter(cycle(train_loader))\n",
    "iterator_test = iter(cycle(test_loader))\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimiser = torch.optim.Adam(logits, lr=0.01)\n",
    "\n",
    "NUM_EPOCHS = 20000  # NB: check for number of training epochs in paper\n",
    "tau = 1  # temperature parameter, NB: check for value in paper\n",
    "alpha = 0.0001/128  # regularisation parameter, NB: check for value in paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7870268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     60\u001b[0m optimiser\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     62\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(otp_pred, otp_)\u001b[38;5;241m.\u001b[39mto(handler\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(all_logits)\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m---> 64\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m optimiser\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     67\u001b[0m loss_hist\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DeepLeaning\\lib\\site-packages\\torch\\_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DeepLeaning\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "loss_hist = []\n",
    "NUM_EPOCHS = 20000\n",
    "for e in range(NUM_EPOCHS):\n",
    "    print(f'Starting epoch {e}...')\n",
    "\n",
    "    '''Sampling and generating masks.'''\n",
    "\n",
    "    U1 = torch.rand(1, requires_grad=True).to(handler.network.device)\n",
    "    U2 = torch.rand(1, requires_grad=True).to(handler.network.device)\n",
    "\n",
    "    samples = []\n",
    "\n",
    "    for layer in logits:\n",
    "        layer.requires_grad_(requires_grad=True)\n",
    "\n",
    "        #         if layer.grad is not None:\n",
    "        #             layer.grad.detach_()\n",
    "        #             layer.grad.zero_()\n",
    "\n",
    "        samples.append(torch.sigmoid((layer - torch.log(torch.log(U1) / torch.log(U2))) / tau))\n",
    "\n",
    "    binaries_stop = []\n",
    "\n",
    "    for layer in samples:\n",
    "        with torch.no_grad():\n",
    "            binaries_stop.append((layer > 0.5).float() - layer)\n",
    "\n",
    "    binaries = []\n",
    "    iterator_samples = iter(samples)\n",
    "\n",
    "    for layer in binaries_stop:\n",
    "        binaries.append(layer + next(iterator_samples))\n",
    "\n",
    "    iterator_binaries = iter(binaries)\n",
    "\n",
    "    for layer in handler.network.layers[0]:\n",
    "        if isinstance(layer, torch.nn.Linear):\n",
    "            layer.weight.data * next(iterator_binaries)\n",
    "\n",
    "    '''Inference with masked network and backpropagation.'''\n",
    "\n",
    "    batch = next(iterator_train)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Load in batch data (not binaries for one-hot input)\n",
    "        inp = torch.stack([torch.stack([b[0], b[1]]) for b in batch])\n",
    "        otp = torch.stack([b[2] for b in batch])\n",
    "        ops = torch.stack([b[3] for b in batch])\n",
    "        # Convert batch data toone-hot representation\n",
    "        inp, otp_ = handler.set_batched_digits(inp, otp, ops)\n",
    "        \n",
    "        inp_ = inp.to(handler.network.device)\n",
    "        otp_ = otp_.to(handler.network.device)\n",
    "        \n",
    "        otp_pred = handler.network(inp_)\n",
    "        #otp_pred.requires_grad = True\n",
    "\n",
    "        \n",
    "    all_logits = alpha*torch.cat([layer.detach().view(-1) for layer in logits]).to(handler.network.device)\n",
    "    optimiser.zero_grad()\n",
    "    \n",
    "    loss = criterion(otp_pred, otp_).to(handler.network.device) + torch.sum(all_logits)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "    loss_hist.append(loss.item())\n",
    "    \n",
    "    if e % 200 == 0:\n",
    "        plt.cla()\n",
    "        plt.clf()\n",
    "        plt.plot(loss_hist)\n",
    "        plt.savefig('liveplot.png')\n",
    "        plt.cla()\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "        torch.save(logits, 'masks/trained_logits_add_mask.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1d9896",
   "metadata": {},
   "source": [
    "## generate mask on multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf6af93",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = 0.8\n",
    "test_split = 1 - train_split\n",
    "\n",
    "data_fp = \"generate_datasets/tmp/digit-data/simple_mul.npy\"\n",
    "data = np.load(data_fp, allow_pickle=True)\n",
    "\n",
    "data_len = len(data)\n",
    "train_split_idx = int(data_len * train_split)\n",
    "train_data = data[:train_split_idx]\n",
    "test_data = data[train_split_idx:]\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=torch.tensor(train_data), batch_size=batchsize, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=torch.Tensor(test_data), batch_size=batchsize, shuffle=True)\n",
    "\n",
    "def cycle(iterable):\n",
    "    while True:\n",
    "        for x in iterable:\n",
    "            yield x\n",
    "\n",
    "iterator_train = iter(cycle(train_loader))\n",
    "iterator_test = iter(cycle(test_loader))\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimiser = torch.optim.Adam(logits, lr=0.01)\n",
    "\n",
    "NUM_EPOCHS = 20000  # NB: check for number of training epochs in paper\n",
    "tau = 1  # temperature parameter, NB: check for value in paper\n",
    "alpha = 0.0001/128  # regularisation parameter, NB: check for value in paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2509565",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_hist = []\n",
    "NUM_EPOCHS = 20000\n",
    "for e in range(NUM_EPOCHS):\n",
    "    print(f'Starting epoch {e}...')\n",
    "\n",
    "    '''Sampling and generating masks.'''\n",
    "\n",
    "    U1 = torch.rand(1, requires_grad=True).to(handler.network.device)\n",
    "    U2 = torch.rand(1, requires_grad=True).to(handler.network.device)\n",
    "\n",
    "    samples = []\n",
    "\n",
    "    for layer in logits:\n",
    "        layer.requires_grad_(requires_grad=True)\n",
    "\n",
    "        #         if layer.grad is not None:\n",
    "        #             layer.grad.detach_()\n",
    "        #             layer.grad.zero_()\n",
    "\n",
    "        samples.append(torch.sigmoid((layer - torch.log(torch.log(U1) / torch.log(U2))) / tau))\n",
    "\n",
    "    binaries_stop = []\n",
    "\n",
    "    for layer in samples:\n",
    "        with torch.no_grad():\n",
    "            binaries_stop.append((layer > 0.5).float() - layer)\n",
    "\n",
    "    binaries = []\n",
    "    iterator_samples = iter(samples)\n",
    "\n",
    "    for layer in binaries_stop:\n",
    "        binaries.append(layer + next(iterator_samples))\n",
    "\n",
    "    iterator_binaries = iter(binaries)\n",
    "\n",
    "    for layer in handler.network.layers[0]:\n",
    "        if isinstance(layer, torch.nn.Linear):\n",
    "            layer.weight.data * next(iterator_binaries)\n",
    "\n",
    "    '''Inference with masked network and backpropagation.'''\n",
    "\n",
    "    batch = next(iterator_train)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Load in batch data (not binaries for one-hot input)\n",
    "        inp = torch.stack([torch.stack([b[0], b[1]]) for b in batch])\n",
    "        otp = torch.stack([b[2] for b in batch])\n",
    "        ops = torch.stack([b[3] for b in batch])\n",
    "        # Convert batch data toone-hot representation\n",
    "        inp, otp_ = handler.set_batched_digits(inp, otp, ops)\n",
    "        \n",
    "        inp_ = inp.to(handler.network.device)\n",
    "        otp_ = otp_.to(handler.network.device)\n",
    "        \n",
    "        otp_pred = handler.network(inp_)\n",
    "\n",
    "        \n",
    "    all_logits = alpha*torch.cat([layer.view(-1) for layer in logits]).to(handler.network.device)\n",
    "    optimiser.zero_grad()\n",
    "    \n",
    "    loss = criterion(otp_pred, otp_).to(handler.network.device) + torch.sum(all_logits)\n",
    "    #loss.requires_grad = True\n",
    "    \n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "    loss_hist.append(loss.item())\n",
    "    \n",
    "    if e % 200 == 0:\n",
    "        plt.cla()\n",
    "        plt.clf()\n",
    "        plt.plot(loss_hist)\n",
    "        plt.savefig('liveplot.png')\n",
    "        plt.cla()\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "        torch.save(logits, 'masks/trained_logits_mul_mask.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f5abc7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
