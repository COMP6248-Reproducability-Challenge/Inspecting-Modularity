{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9f73235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models.addmul import HandleAddMul\n",
    "\n",
    "def cycle(iterable):\n",
    "    while True:\n",
    "        for x in iterable:\n",
    "            yield x\n",
    "\n",
    "network_cache_dir = \"networks/cache-networks/\"\n",
    "network_name = \"lyr256-split0.8-lr0.01-add-mul.data\"\n",
    "\n",
    "checkpoint = True\n",
    "test_flag = 1\n",
    "\n",
    "input_dims = [42]\n",
    "output_dims = [20]\n",
    "batchsize = 128\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd90f94",
   "metadata": {},
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ea41ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... FNN Network training on cuda:0 ...\n",
      "Accessing : networks/cache-networks/lyr256-split0.8-lr0.01-add-mul.data\n",
      "networks/cache-networks/lyr256-split0.8-lr0.01-add-mul.data\n"
     ]
    }
   ],
   "source": [
    "'''Initialise logits & define loss and Optimiser'''\n",
    "handler = HandleAddMul(input_dims, output_dims, dir=network_cache_dir+network_name, \n",
    "                       checkpoint=checkpoint, use_optimiser=False)\n",
    "\n",
    "logits = torch.load('masks/trained_logits_add_mask_.pt')\n",
    "binary_mask = []\n",
    "with torch.no_grad():\n",
    "    for layer in logits:\n",
    "        binary = (torch.sigmoid(layer) > 0.5).float()\n",
    "        binary_mask.append(binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64baddaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = 0.8\n",
    "test_split = 1 - train_split\n",
    "\n",
    "data_fp = [\"generate_datasets/tmp/digit-data/simple_add.npy\",\n",
    "           \"generate_datasets/tmp/digit-data/simple_mul.npy\"]\n",
    "data_add = np.load(data_fp[0], allow_pickle=True)\n",
    "data_mul = np.load(data_fp[1], allow_pickle=True)\n",
    "data_add_len = len(data_add)\n",
    "data_mul_len = len(data_mul)\n",
    "\n",
    "train_split_idx_add = int(data_add_len * train_split)\n",
    "train_split_idx_mul = int(data_mul_len * train_split)\n",
    "test_data_add = data_add[train_split_idx_add:]\n",
    "test_data_mul = data_mul[train_split_idx_mul:]\n",
    "\n",
    "test_loader_add = torch.utils.data.DataLoader(dataset=torch.Tensor(test_data_add), batch_size=batchsize, shuffle=True)\n",
    "test_loader_mul = torch.utils.data.DataLoader(dataset=torch.Tensor(test_data_mul), batch_size=batchsize, shuffle=True)\n",
    "\n",
    "def cycle(iterable):\n",
    "    while True:\n",
    "        for x in iterable:\n",
    "            yield x\n",
    "\n",
    "iterator_test_add = iter(cycle(test_loader_add))\n",
    "iterator_test_mul = iter(cycle(test_loader_mul))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d62bd087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0...\n",
      "Starting epoch 1...\n",
      "Starting epoch 2...\n",
      "Starting epoch 3...\n",
      "Starting epoch 4...\n",
      "Starting epoch 5...\n",
      "Starting epoch 6...\n",
      "Starting epoch 7...\n",
      "Starting epoch 8...\n",
      "Starting epoch 9...\n",
      "Starting epoch 10...\n",
      "Starting epoch 11...\n",
      "Starting epoch 12...\n",
      "Starting epoch 13...\n",
      "Starting epoch 14...\n",
      "Starting epoch 15...\n",
      "Starting epoch 16...\n",
      "Starting epoch 17...\n",
      "Starting epoch 18...\n",
      "Starting epoch 19...\n",
      "Starting epoch 20...\n",
      "Starting epoch 21...\n",
      "Starting epoch 22...\n",
      "Starting epoch 23...\n",
      "Starting epoch 24...\n",
      "Starting epoch 25...\n",
      "Starting epoch 26...\n",
      "Starting epoch 27...\n",
      "Starting epoch 28...\n",
      "Starting epoch 29...\n",
      "Starting epoch 30...\n",
      "Starting epoch 31...\n",
      "Starting epoch 32...\n",
      "Starting epoch 33...\n",
      "Starting epoch 34...\n",
      "Starting epoch 35...\n",
      "Starting epoch 36...\n",
      "Starting epoch 37...\n",
      "Starting epoch 38...\n",
      "Starting epoch 39...\n",
      "Starting epoch 40...\n",
      "Starting epoch 41...\n",
      "Starting epoch 42...\n",
      "Starting epoch 43...\n",
      "Starting epoch 44...\n",
      "Starting epoch 45...\n",
      "Starting epoch 46...\n",
      "Starting epoch 47...\n",
      "Starting epoch 48...\n",
      "Starting epoch 49...\n",
      "Starting epoch 50...\n",
      "Starting epoch 51...\n",
      "Starting epoch 52...\n",
      "Starting epoch 53...\n",
      "Starting epoch 54...\n",
      "Starting epoch 55...\n",
      "Starting epoch 56...\n",
      "Starting epoch 57...\n",
      "Starting epoch 58...\n",
      "Starting epoch 59...\n",
      "Starting epoch 60...\n",
      "Starting epoch 61...\n",
      "Starting epoch 62...\n",
      "Starting epoch 63...\n",
      "Starting epoch 64...\n",
      "Starting epoch 65...\n",
      "Starting epoch 66...\n",
      "Starting epoch 67...\n",
      "Starting epoch 68...\n",
      "Starting epoch 69...\n",
      "Starting epoch 70...\n",
      "Starting epoch 71...\n",
      "Starting epoch 72...\n",
      "Starting epoch 73...\n",
      "Starting epoch 74...\n",
      "Starting epoch 75...\n",
      "Starting epoch 76...\n",
      "Starting epoch 77...\n",
      "Starting epoch 78...\n",
      "Starting epoch 79...\n",
      "Starting epoch 80...\n",
      "Starting epoch 81...\n",
      "Starting epoch 82...\n",
      "Starting epoch 83...\n",
      "Starting epoch 84...\n",
      "Starting epoch 85...\n",
      "Starting epoch 86...\n",
      "Starting epoch 87...\n",
      "Starting epoch 88...\n",
      "Starting epoch 89...\n",
      "Starting epoch 90...\n",
      "Starting epoch 91...\n",
      "Starting epoch 92...\n",
      "Starting epoch 93...\n",
      "Starting epoch 94...\n",
      "Starting epoch 95...\n",
      "Starting epoch 96...\n",
      "Starting epoch 97...\n",
      "Starting epoch 98...\n",
      "Starting epoch 99...\n"
     ]
    }
   ],
   "source": [
    "loss_hist = []\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "mul_acc = 0.\n",
    "add_acc = 0.\n",
    "\n",
    "for e in range(NUM_EPOCHS):\n",
    "    print(f'Starting epoch {e}...')\n",
    "    '''Reload weights'''\n",
    "    handler.network.load_save()\n",
    "    handler.refreeze_weights()\n",
    "    \n",
    "    '''Call Addition Mask'''\n",
    "    batch_add = next(iterator_test_add)\n",
    "    batch_mul = next(iterator_test_mul)\n",
    "    iterator_binary = iter(cycle(binary_mask))\n",
    "    \n",
    "    batches = [batch_add, batch_mul]\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(batches):\n",
    "            inp = torch.stack([torch.stack([b[0], b[1]]) for b in batch])\n",
    "            otp = torch.stack([b[2] for b in batch]).to(handler.network.device)\n",
    "            ops = torch.stack([b[3] for b in batch])\n",
    "            # Convert batch data toone-hot representation\n",
    "            inp, otp_ = handler.set_batched_digits(inp, otp, ops)\n",
    "            inp_ = inp.to(handler.network.device)\n",
    "            otp_ = otp_.to(handler.network.device)\n",
    "            '''Pass batch data through masked net'''\n",
    "            idx = 0\n",
    "            for layer in handler.network.layers[0]:\n",
    "                if isinstance(layer, torch.nn.Linear):\n",
    "                    b = layer.bias\n",
    "                    w = layer.weight\n",
    "                    m = next(iterator_binary)\n",
    "                    inp_ = handler.network.forward_mask_layer(inp_, m, w, b)\n",
    "                    idx+=1\n",
    "                else:\n",
    "                    inp_ = layer(inp_)\n",
    "            otp_pred = inp_       \n",
    "            \n",
    "            otp_stck = torch.stack([otp_pred[:,:10], otp_pred[:,10:]])\n",
    "            otp_argmax = torch.argmax(otp_stck, dim=2)\n",
    "            otp_class = otp_argmax[0]*10 + otp_argmax[1]\n",
    "            diff = otp_class - otp\n",
    "            cnt = len(diff[abs(diff) == 0])\n",
    "            otp_argmax = torch.argmax(otp_stck, dim=2)\n",
    "            otp_class = otp_argmax[0]*10 + otp_argmax[1]\n",
    "            diff = otp_class - otp\n",
    "            cnt = len(diff[abs(diff) == 0])\n",
    "        \n",
    "            if not idx:\n",
    "                add_acc += cnt/float((len(diff)))\n",
    "            else: \n",
    "                mul_acc += cnt/float((len(diff)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5906a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Add Accuracy: {add_acc/NUM_EPOCHS} \\n Mul Accuracy: {mul_acc/NUM_EPOCHS}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
