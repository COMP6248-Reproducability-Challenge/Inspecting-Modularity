{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0788e6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models.addmul import HandleAddMul"
   ]
  },
  {
   "cell_type": "raw",
   "id": "96b29cec",
   "metadata": {},
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37a4f0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... FNN Network training on cuda:0 ...\n",
      "Accessing : networks/cache-networks/lyr256-split0.8-lr0.01-mul.data\n",
      "networks/cache-networks/lyr256-split0.8-lr0.01-mul.data\n",
      "Load saves ...\n"
     ]
    }
   ],
   "source": [
    "network_cache_dir = \"networks/cache-networks/\"\n",
    "network_name = \"lyr256-split0.8-lr0.01-mul.data\"\n",
    "\n",
    "checkpoint = True\n",
    "test_flag = 1\n",
    "\n",
    "input_dims = [42]\n",
    "output_dims = [20]\n",
    "batchsize = 128\n",
    "num_epochs = 1\n",
    "\n",
    "handler = HandleAddMul(input_dims, output_dims, dir=network_cache_dir + network_name, checkpoint=checkpoint, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "225b7427",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = []\n",
    "for layer in handler.network.layers[0]:\n",
    "    if isinstance(layer, torch.nn.Linear):\n",
    "        logits.append(torch.full_like(layer.weight.data.clone(), 0.9, requires_grad=True))\n",
    "\n",
    "for name, param in handler.network.named_parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbaf9e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = 0.8\n",
    "test_split = 1 - train_split\n",
    "\n",
    "data_fp = \"generate_datasets/tmp/digit-data/simple_add.npy\"\n",
    "data = np.load(data_fp, allow_pickle=True)\n",
    "\n",
    "data_len = len(data)\n",
    "train_split_idx = int(data_len * train_split)\n",
    "train_data = data[:train_split_idx]\n",
    "test_data = data[train_split_idx:]\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=torch.tensor(train_data), batch_size=batchsize, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=torch.Tensor(test_data), batch_size=batchsize, shuffle=True)\n",
    "\n",
    "def cycle(iterable):\n",
    "    while True:\n",
    "        for x in iterable:\n",
    "            yield x\n",
    "\n",
    "iterator_train = iter(cycle(train_loader))\n",
    "iterator_test = iter(test_loader)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "optimiser = torch.optim.Adam(logits, lr=0.01)\n",
    "\n",
    "NUM_EPOCHS = 20000  # NB: check for number of training epochs in paper\n",
    "tau = 1  # temperature parameter, NB: check for value in paper\n",
    "alpha = 0.0001/128  # regularisation parameter, NB: check for value in paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90b0e0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = 0.8\n",
    "test_split = 1 - train_split\n",
    "\n",
    "data_fp = \"generate_datasets/tmp/digit-data/simple_add.npy\"\n",
    "data = np.load(data_fp, allow_pickle=True)\n",
    "\n",
    "data_len = len(data)\n",
    "train_split_idx = int(data_len * train_split)\n",
    "train_data = data[:train_split_idx]\n",
    "test_data = data[train_split_idx:]\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=torch.tensor(train_data), batch_size=batchsize, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=torch.Tensor(test_data), batch_size=batchsize, shuffle=True)\n",
    "\n",
    "iterator_train = iter(train_loader)\n",
    "iterator_test = iter(test_loader)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimiser = torch.optim.Adam(logits, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e095969",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'SummaryWriter' from 'tensorboard' (C:\\Users\\azzad\\anaconda3\\envs\\DeepLeaning\\lib\\site-packages\\tensorboard\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SummaryWriter\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'SummaryWriter' from 'tensorboard' (C:\\Users\\azzad\\anaconda3\\envs\\DeepLeaning\\lib\\site-packages\\tensorboard\\__init__.py)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6539bbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     61\u001b[0m all_logits \u001b[38;5;241m=\u001b[39m alpha\u001b[38;5;241m*\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcat([layer\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m logits])\u001b[38;5;241m.\u001b[39mto(handler\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     62\u001b[0m optimiser\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 64\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43motp_target\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(handler\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(all_logits)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m#loss.requires_grad = True\u001b[39;00m\n\u001b[0;32m     67\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DeepLeaning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DeepLeaning\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1163\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1164\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1165\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DeepLeaning\\lib\\site-packages\\torch\\nn\\functional.py:2996\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   2994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2995\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 2996\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "loss_hist = []\n",
    "for e in range(NUM_EPOCHS):\n",
    "    print(f'Starting epoch {e}...')\n",
    "\n",
    "    '''Sampling and generating masks.'''\n",
    "\n",
    "    U1 = torch.rand(1, requires_grad=True).to(handler.network.device)\n",
    "    U2 = torch.rand(1, requires_grad=True).to(handler.network.device)\n",
    "\n",
    "    samples = []\n",
    "\n",
    "    for layer in logits:\n",
    "        layer.requires_grad_(requires_grad=True)\n",
    "\n",
    "        #         if layer.grad is not None:\n",
    "        #             layer.grad.detach_()\n",
    "        #             layer.grad.zero_()\n",
    "\n",
    "        samples.append(torch.sigmoid((layer - torch.log(torch.log(U1) / torch.log(U2))) / tau))\n",
    "\n",
    "    binaries_stop = []\n",
    "\n",
    "    for layer in samples:\n",
    "        with torch.no_grad():\n",
    "            binaries_stop.append((layer > 0.5).float() - layer)\n",
    "\n",
    "    binaries = []\n",
    "    iterator_samples = iter(samples)\n",
    "\n",
    "    for layer in binaries_stop:\n",
    "        binaries.append(layer + next(iterator_samples))\n",
    "\n",
    "    iterator_binaries = iter(binaries)\n",
    "\n",
    "    for layer in handler.network.layers[0]:\n",
    "        if isinstance(layer, torch.nn.Linear):\n",
    "            with torch.no_grad():\n",
    "                layer.weight.data * next(iterator_binaries)\n",
    "\n",
    "    '''Inference with masked network and backpropagation.'''\n",
    "\n",
    "    batch = next(iterator_train)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inp = torch.stack([torch.stack([b[0], b[1]]) for b in batch])\n",
    "        otp = torch.stack([b[2] for b in batch])\n",
    "        ops = torch.stack([b[3] for b in batch])\n",
    "        inp, otp_ = handler.set_batched_digits(inp, otp, ops)\n",
    "        inp_ = inp.to(handler.network.device)\n",
    "        otp_ = otp_.to(handler.network.device)\n",
    "\n",
    "        otp_pred = handler.network(inp_)\n",
    "\n",
    "        pred = torch.stack((otp_pred[:,0:10], otp_pred[:,10:]))\n",
    "        pred = torch.argmax(pred, dim=2)\n",
    "        pred[0,:] = pred[0,:]*10\n",
    "        pred = torch.sum(pred, dim=0).to(handler.network.device)\n",
    "\n",
    "        otp_target = otp.float().to(handler.network.device)\n",
    "\n",
    "    all_logits = alpha*torch.cat([layer.view(-1) for layer in logits]).to(handler.network.device)\n",
    "    optimiser.zero_grad()\n",
    "    \n",
    "    loss = criterion(pred, otp_target).to(handler.network.device) + torch.sum(all_logits)\n",
    "    #loss.requires_grad = True\n",
    "    \n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "    loss_hist.append(loss.item())\n",
    "\n",
    "    if e % 200 == 0:\n",
    "        plt.cla()\n",
    "        plt.clf()\n",
    "        plt.plot(loss_hist)\n",
    "        plt.savefig('liveplot.png')\n",
    "        plt.cla()\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "        torch.save(logits, 'trainbd_logits.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bab722c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m load_logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrained_logits.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mload_logits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m())\n\u001b[0;32m      3\u001b[0m sig_logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(load_logits)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "load_logits = torch.load('trained_logits.pt')\n",
    "\n",
    "# Sigmoid(l_i) rather than sigmoid(T(li-log(logU1/logU2)))\n",
    "sig_logits = []\n",
    "for layer in load_logits:\n",
    "    sig_logits.append(torch.sigmoid(layer))\n",
    "\n",
    "for layer in samples:\n",
    "    with torch.no_grad():\n",
    "        binaries_stop.append((layer > 0.5).float() - layer)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
